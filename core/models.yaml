- capabilities:
  - tools
  description: The latest GPT-3.5 Turbo model with higher accuracy at responding in requested formats and a fix for a bug which caused a text encoding issue for non-English language function calls.
  image_price: null
  inference_provider:
    model_name: gpt-3.5-turbo-0125
    provider: openai
  input_formats:
  - text
  limits:
    max_context_size: 16385
  model: gpt-3.5-turbo-0125
  model_provider: openai
  output_formats:
  - text
  price:
    per_input_token: 0.5
    per_output_token: 1.5
    valid_from: null
  type: completions
- capabilities:
  - tools
  description: High-intelligence flagship model for complex, multi-step tasks. GPT-4o is cheaper and faster than GPT-4 Turbo. It is multimodal (accepting text or image inputs and outputting text), and it has the same high intelligence as GPT-4 Turbo but is much more efficient—it generates text 2x faster and is 50% cheaper. Additionally, GPT-4o has the best vision and performance across non-English languages of any of our models. GPT-4o is available in the OpenAI API to paying customers.
  image_price: null
  inference_provider:
    model_name: gpt-4o
    provider: openai
  input_formats:
  - text
  - image
  limits:
    max_context_size: 128000
  model: gpt-4o
  model_provider: openai
  output_formats:
  - text
  price:
    per_input_token: 2.5
    per_output_token: 10.0
    valid_from: null
  type: completions
- capabilities: []
  description: DBRX Instruct is a model by Databricks, designed for instruction-following tasks and general language understanding.
  image_price: null
  inference_provider:
    model_name: databricks/dbrx-instruct
    provider: togetherai
  input_formats:
  - text
  limits:
    max_context_size: 32768
  model: dbrx-instruct
  model_provider: togetherai
  output_formats:
  - text
  price:
    per_input_token: 0.8
    per_output_token: 0.8
    valid_from: null
  type: completions
- capabilities: []
  description: MythoMax-L2 (13B) is a model by Gryphe, known for its creative text generation capabilities.
  image_price: null
  inference_provider:
    model_name: Gryphe/MythoMax-L2-13b
    provider: togetherai
  input_formats:
  - text
  limits:
    max_context_size: 4096
  model: MythoMax-L2-13b
  model_provider: togetherai
  output_formats:
  - text
  price:
    per_input_token: 0.3
    per_output_token: 0.3
    valid_from: null
  type: completions
- capabilities:
  - tools
  description: GPT-4o mini (o for omni) is a fast, affordable small model for focused tasks. It accepts both text and image inputs, and produces text outputs (including Structured Outputs). It is ideal for fine-tuning, and model outputs from a larger model like GPT-4o can be distilled to GPT-4o-mini to produce similar results at lower cost and latency.The knowledge cutoff for GPT-4o-mini models is October, 2023.
  image_price: null
  inference_provider:
    model_name: gpt-4o-mini
    provider: openai
  input_formats:
  - text
  - image
  limits:
    max_context_size: 128000
  model: gpt-4o-mini
  model_provider: openai
  output_formats:
  - text
  price:
    per_input_token: 0.15
    per_output_token: 0.6
    valid_from: null
  type: completions
- capabilities:
  - tools
  description: The o1 series of large language models are trained with reinforcement learning to perform complex reasoning. o1 models think before they answer, producing a long internal chain of thought before responding to the user. Reasoning model designed to solve hard problems across domains
  image_price: null
  inference_provider:
    model_name: o1-preview
    provider: openai
  input_formats:
  - text
  limits:
    max_context_size: 128000
  model: o1-preview
  model_provider: openai
  output_formats:
  - text
  price:
    per_input_token: 15.0
    per_output_token: 7.5
    valid_from: null
  type: completions
- capabilities: []
  description: The o1 series of large language models are trained with reinforcement learning to perform complex reasoning. o1 models think before they answer, producing a long internal chain of thought before responding to the user. Faster and cheaper reasoning model particularly good at coding, math, and science
  image_price: null
  inference_provider:
    model_name: o1-mini
    provider: openai
  input_formats:
  - text
  limits:
    max_context_size: 128000
  model: o1-mini
  model_provider: openai
  output_formats:
  - text
  price:
    per_input_token: 3.0
    per_output_token: 12.0
    valid_from: null
  type: completions
- capabilities: []
  description: Most capable 2nd generation embedding model, replacing 16 first generation models
  image_price: null
  inference_provider:
    model_name: text-embedding-ada-002
    provider: openai
  input_formats:
  - text
  limits:
    max_context_size: 8192
  model: text-embedding-ada-002
  model_provider: openai
  output_formats:
  - text
  price:
    per_input_token: 0.1
    per_output_token: 0.0
    valid_from: null
  type: embeddings
- capabilities: []
  description: Increased performance over 2nd generation ada embedding model
  image_price: null
  inference_provider:
    model_name: text-embedding-3-small
    provider: openai
  input_formats:
  - text
  limits:
    max_context_size: 8191
  model: text-embedding-3-small
  model_provider: openai
  output_formats:
  - text
  price:
    per_input_token: 0.02
    per_output_token: 0.0
    valid_from: null
  type: embeddings
- capabilities: []
  description: Most capable embedding model for both english and non-english tasks
  image_price: null
  inference_provider:
    model_name: text-embedding-3-large
    provider: openai
  input_formats:
  - text
  limits:
    max_context_size: 8191
  model: text-embedding-3-large
  model_provider: openai
  output_formats:
  - text
  price:
    per_input_token: 0.13
    per_output_token: 0.0
    valid_from: null
  type: embeddings
- capabilities:
  - tools
  description: instruction-following conversational model that performs language tasks with high quality, more reliably and with a longer context than our base generative models.
  image_price: null
  inference_provider:
    model_name: command-r-v1:0
    provider: bedrock
  input_formats:
  - text
  limits:
    max_context_size: 128000
  model: command-r-v1:0
  model_provider: cohere
  output_formats:
  - text
  price:
    per_input_token: 0.5
    per_output_token: 1.5
    valid_from: null
  type: completions
- capabilities:
  - tools
  description: Command R+ is an instruction-following conversational model that performs language tasks at a higher quality, more reliably, and with a longer context than previous models. It is best suited for complex RAG workflows and multi-step tool use.
  image_price: null
  inference_provider:
    model_name: command-r-plus-v1:0
    provider: bedrock
  input_formats:
  - text
  limits:
    max_context_size: 128000
  model: command-r-plus-v1:0
  model_provider: cohere
  output_formats:
  - text
  price:
    per_input_token: 3.0
    per_output_token: 15.0
    valid_from: null
  type: completions
- capabilities:
  - tools
  description: Powerful model for highly complex tasks. Top-level intelligence, fluency, and understanding
  image_price: null
  inference_provider:
    model_name: claude-3-opus-20240229
    provider: anthropic
  input_formats:
  - text
  - image
  limits:
    max_context_size: 200000
  model: claude-3-opus-20240229
  model_provider: anthropic
  output_formats:
  - text
  price:
    per_input_token: 5.0
    per_output_token: 75.0
    valid_from: null
  type: completions
- capabilities:
  - tools
  description: Balance of intelligence and speed. Strong utility, balanced for scaled deployments
  image_price: null
  inference_provider:
    model_name: claude-3-sonnet-20240229
    provider: anthropic
  input_formats:
  - text
  - image
  limits:
    max_context_size: 200000
  model: claude-3-sonnet-20240229
  model_provider: anthropic
  output_formats:
  - text
  price:
    per_input_token: 3.0
    per_output_token: 15.0
    valid_from: null
  type: completions
- capabilities:
  - tools
  description: Fastest and most compact model for near-instant responsiveness. Quick and accurate targeted performance
  image_price: null
  inference_provider:
    model_name: claude-3-haiku-20240307
    provider: anthropic
  input_formats:
  - text
  - image
  limits:
    max_context_size: 200000
  model: claude-3-haiku-20240307
  model_provider: anthropic
  output_formats:
  - text
  price:
    per_input_token: 0.25
    per_output_token: 1.25
    valid_from: null
  type: completions
- capabilities:
  - tools
  description: Claude most intelligent model. Highest level of intelligence and capability
  image_price: null
  inference_provider:
    model_name: claude-3-5-sonnet-20240620
    provider: anthropic
  input_formats:
  - text
  - image
  limits:
    max_context_size: 200000
  model: claude-3-5-sonnet-20240620
  model_provider: anthropic
  output_formats:
  - text
  price:
    per_input_token: 3.0
    per_output_token: 15.0
    valid_from: null
  type: completions
- capabilities:
  - tools
  description: Fast and versatile performance across a diverse variety of tasks.
  image_price: null
  inference_provider:
    model_name: gemini-1.5-flash-latest
    provider: gemini
  input_formats:
  - text
  - image
  - audio
  - video
  limits:
    max_context_size: 1000000
  model: gemini-1.5-flash-latest
  model_provider: gemini
  output_formats:
  - text
  price:
    per_input_token: 0.15
    per_output_token: 0.6
    valid_from: null
  type: completions
- capabilities:
  - tools
  description: lightweight model, smaller and faster, lower price + higher rate limits + Lower latency on small prompts (compared to 1.5 Flash)
  image_price: null
  inference_provider:
    model_name: gemini-1.5-flash-8b
    provider: gemini
  input_formats:
  - text
  - image
  - audio
  - video
  limits:
    max_context_size: 1000000
  model: gemini-1.5-flash-8b
  model_provider: gemini
  output_formats:
  - text
  price:
    per_input_token: 0.075
    per_output_token: 0.3
    valid_from: null
  type: completions
- capabilities:
  - tools
  description: lightweight model, smaller and faster, lower price + higher rate limits + Lower latency on small prompts (compared to 1.5 Flash)
  image_price: null
  inference_provider:
    model_name: gemini-1.5-pro-latest
    provider: gemini
  input_formats:
  - text
  - image
  - audio
  - video
  limits:
    max_context_size: 2000000
  model: gemini-1.5-pro-latest
  model_provider: gemini
  output_formats:
  - text
  price:
    per_input_token: 2.5
    per_output_token: 10.0
    valid_from: null
  type: completions
- capabilities:
  - tools
  description: Ideal for limited computational power and resources, faster training times, and edge devices.
  image_price: null
  inference_provider:
    model_name: llama3-1-8b-instruct-v1.0
    provider: bedrock
  input_formats:
  - text
  limits:
    max_context_size: 128000
  model: llama3-1-8b-instruct-v1.0
  model_provider: meta
  output_formats:
  - text
  price:
    per_input_token: 0.22
    per_output_token: 0.22
    valid_from: null
  type: completions
- capabilities:
  - tools
  description: Ideal for content creation, conversational AI, language understanding, research development, and enterprise applications. With new latency-optimized inference capabilities available in public preview, this model sets a new performance benchmark for AI solutions that process extensive text inputs, enabling applications to respond more quickly and handle longer queries more efficiently.
  image_price: null
  inference_provider:
    model_name: llama3-1-70b-instruct-v1.0
    provider: bedrock
  input_formats:
  - text
  limits:
    max_context_size: 128000
  model: llama3-1-70b-instruct-v1.0
  model_provider: meta
  output_formats:
  - text
  price:
    per_input_token: 0.72
    per_output_token: 0.72
    valid_from: null
  type: completions
- capabilities: []
  description: A 7B dense Transformer, fast-deployed and easily customizable. Small, yet powerful for a variety of use cases.
  image_price: null
  inference_provider:
    model_name: mistral-7b-instruct-v0.2
    provider: bedrock
  input_formats:
  - text
  limits:
    max_context_size: 32000
  model: mistral-7b-instruct-v0.2
  model_provider: mistral
  output_formats:
  - text
  price:
    per_input_token: 0.15
    per_output_token: 0.2
    valid_from: null
  type: completions
- capabilities: []
  description: 'ideal for content creation, conversational AI, language understanding, research development, and enterprise applications. '
  image_price: null
  inference_provider:
    model_name: llama3-70b-instruct-v1.0
    provider: bedrock
  input_formats:
  - text
  limits:
    max_context_size: 32000
  model: llama3-70b-instruct-v1.0
  model_provider: meta
  output_formats:
  - text
  price:
    per_input_token: 2.65
    per_output_token: 3.5
    valid_from: null
  type: completions
- capabilities: []
  description: Text-only lightweight model built to deliver highly accurate and relevant results. Designed for applications requiring low-latency inferencing and limited computational resources. Ideal for query and prompt rewriting, mobile AI-powered writing assistants, and customer service applications, particularly on edge devices where its efficiency and low latency enable seamless integration into various applications, including mobile AI-powered writing assistants and customer service chatbots.
  image_price: null
  inference_provider:
    model_name: llama3-2-3b-instruct-v1.0
    provider: bedrock
  input_formats:
  - text
  - image
  limits:
    max_context_size: 128000
  model: llama3-2-3b-instruct-v1.0
  model_provider: meta
  output_formats:
  - text
  price:
    per_input_token: 0.15
    per_output_token: 0.15
    valid_from: null
  type: completions
- capabilities: []
  description: ideal for limited computational power and resources, and edge devices. The model excels at text summarization, text classification, sentiment analysis, and language translation.
  image_price: null
  inference_provider:
    model_name: llama3-8b-instruct-v1.0
    provider: bedrock
  input_formats:
  - text
  limits:
    max_context_size: 32000
  model: llama3-8b-instruct-v1.0
  model_provider: meta
  output_formats:
  - text
  price:
    per_input_token: 0.3
    per_output_token: 0.6
    valid_from: null
  type: completions
- capabilities: []
  description: Text-only lightweight model built to deliver fast and accurate responses. Ideal for edge devices and mobile applications. The model enables on-device AI capabilities while preserving user privacy and minimizing latency.
  image_price: null
  inference_provider:
    model_name: llama3-2-1b-instruct-v1.0
    provider: bedrock
  input_formats:
  - text
  - image
  limits:
    max_context_size: 128000
  model: llama3-2-1b-instruct-v1.0
  model_provider: meta
  output_formats:
  - text
  price:
    per_input_token: 0.1
    per_output_token: 0.1
    valid_from: null
  type: completions
- capabilities: []
  description: A 7B sparse Mixture-of-Experts model with stronger capabilities than Mistral AI 7B. Uses 12B active parameters out of 45B total.
  image_price: null
  inference_provider:
    model_name: mixtral-8x7b-instruct-v0.1
    provider: bedrock
  input_formats:
  - text
  limits:
    max_context_size: 32000
  model: mixtral-8x7b-instruct-v0.1
  model_provider: mistral
  output_formats:
  - text
  price:
    per_input_token: 0.45
    per_output_token: 0.7
    valid_from: null
  type: completions
- capabilities:
  - tools
  description: Next generation features, speed, and multimodal generation for a diverse variety of tasks
  image_price: null
  inference_provider:
    model_name: gemini-2.0-flash-exp
    provider: gemini
  input_formats:
  - text
  - image
  - audio
  limits:
    max_context_size: 1048576
  model: gemini-2.0-flash-exp
  model_provider: gemini
  output_formats:
  - text
  price:
    per_input_token: 0.15
    per_output_token: 0.6
    valid_from: null
  type: completions
- capabilities:
  - tools
  description: Grok-beta is an experimental AI model developed by xAI, designed to provide insightful, witty, and context-aware responses while continuously learning and improving through user interactions.
  image_price: null
  inference_provider:
    model_name: grok-beta
    provider: xai
  input_formats:
  - text
  limits:
    max_context_size: 131072
  model: grok-beta
  model_provider: xai
  output_formats:
  - text
  price:
    per_input_token: 5.0
    per_output_token: 15.0
    valid_from: null
  type: completions
- capabilities:
  - tools
  description: Grok-2-vision-1212 is an advanced AI model developed by xAI that integrates multimodal capabilities, allowing it to process and understand both text and visual inputs to provide more comprehensive and context-aware responses
  image_price: null
  inference_provider:
    model_name: grok-2-vision-1212
    provider: xai
  input_formats:
  - text
  - image
  limits:
    max_context_size: 32768
  model: grok-2-vision-1212
  model_provider: xai
  output_formats:
  - text
  price:
    per_input_token: 2.0
    per_output_token: 10.0
    valid_from: null
  type: completions
- capabilities:
  - tools
  description: DeepSeek-Chat is an advanced conversational AI model designed to provide intelligent
  image_price: null
  inference_provider:
    model_name: deepseek-chat
    provider: deepseek
  input_formats:
  - text
  limits:
    max_context_size: 64000
  model: deepseek-chat
  model_provider: deepseek
  output_formats:
  - text
  price:
    per_input_token: 0.14
    per_output_token: 0.28
    valid_from: null
  type: completions
- capabilities:
  - tools
  description: Grok-vision-beta is an advanced AI model developed by xAI that integrates multimodal capabilities, allowing it to process and understand both text and visual inputs to provide more comprehensive and context-aware responses
  image_price: null
  inference_provider:
    model_name: grok-vision-beta
    provider: xai
  input_formats:
  - text
  - image
  limits:
    max_context_size: 8192
  model: grok-vision-beta
  model_provider: xai
  output_formats:
  - text
  price:
    per_input_token: 5.0
    per_output_token: 15.0
    valid_from: null
  type: completions
- capabilities:
  - tools
  description: Grok-2 is an advanced AI model developed by xAI, designed to provide highly accurate and helpful responses to a wide range of questions, often with a unique perspective on humanity.
  image_price: null
  inference_provider:
    model_name: grok-2
    provider: xai
  input_formats:
  - text
  limits:
    max_context_size: 131072
  model: grok-2
  model_provider: xai
  output_formats:
  - text
  price:
    per_input_token: 2.0
    per_output_token: 10.0
    valid_from: null
  type: completions
- capabilities: []
  description: Llama 3.3 70B is a text-only instruction-tuned model that provides enhanced performance relative to Llama 3.1 70B–and to Llama 3.2 90B when used for text-only applications.
  image_price: null
  inference_provider:
    model_name: llama3-3-70b-instruct-v1.0
    provider: bedrock
  input_formats:
  - text
  limits:
    max_context_size: 128000
  model: llama3-3-70b-instruct-v1.0
  model_provider: meta
  output_formats:
  - text
  price:
    per_input_token: 0.72
    per_output_token: 0.72
    valid_from: null
  type: completions
- capabilities: []
  description: Qwen 2 Instruct (72B) is a large-scale model by Qwen, offering advanced capabilities for complex language tasks.
  image_price: null
  inference_provider:
    model_name: Qwen/Qwen2-72B-Instruct
    provider: togetherai
  input_formats:
  - text
  limits:
    max_context_size: 32768
  model: Qwen2-72B-Instruct
  model_provider: togetherai
  output_formats:
  - text
  price:
    per_input_token: 1.2
    per_output_token: 1.2
    valid_from: null
  type: completions
- capabilities: []
  description: Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools.
  image_price: null
  inference_provider:
    model_name: google/gemma-2-9b-it
    provider: togetherai
  input_formats:
  - text
  limits:
    max_context_size: 8192
  model: gemma-2-9b-it
  model_provider: togetherai
  output_formats:
  - text
  price:
    per_input_token: 0.3
    per_output_token: 0.3
    valid_from: null
  type: completions
- capabilities: []
  description: Llama-3.1-Nemotron-70B-Instruct is a large language model customized by NVIDIA in order to improve the helpfulness of LLM generated responses.
  image_price: null
  inference_provider:
    model_name: nvidia/Llama-3.1-Nemotron-70B-Instruct-HF
    provider: togetherai
  input_formats:
  - text
  limits:
    max_context_size: 32768
  model: Llama-3.1-Nemotron-70B-Instruct-HF
  model_provider: togetherai
  output_formats:
  - text
  price:
    per_input_token: 0.9
    per_output_token: 0.9
    valid_from: null
  type: completions
- capabilities: []
  description: Qwen 2.5 7B Instruct is a fast and efficient model by Qwen, optimized for quick responses in instruction-following tasks.
  image_price: null
  inference_provider:
    model_name: Qwen/Qwen2.5-7B-Instruct-Turbo
    provider: togetherai
  input_formats:
  - text
  limits:
    max_context_size: 32768
  model: Qwen2.5-7B-Instruct-Turbo
  model_provider: togetherai
  output_formats:
  - text
  price:
    per_input_token: 0.3
    per_output_token: 0.3
    valid_from: null
  type: completions
- capabilities: []
  description: Nous Hermes 2 - Mixtral 8x7B-DPO (46.7B) is a large model by NousResearch, utilizing advanced training techniques for improved performance.
  image_price: null
  inference_provider:
    model_name: NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO
    provider: togetherai
  input_formats:
  - text
  limits:
    max_context_size: 32768
  model: Nous-Hermes-2-Mixtral-8x7B-DPO
  model_provider: togetherai
  output_formats:
  - text
  price:
    per_input_token: 0.9
    per_output_token: 0.9
    valid_from: null
  type: completions
- capabilities: []
  description: Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools.
  image_price: null
  inference_provider:
    model_name: google/gemma-2-27b-it
    provider: togetherai
  input_formats:
  - text
  limits:
    max_context_size: 8192
  model: gemma-2-27b-it
  model_provider: togetherai
  output_formats:
  - text
  price:
    per_input_token: 0.3
    per_output_token: 0.3
    valid_from: null
  type: completions
- capabilities: []
  description: WizardLM-2 8x22B is a large language model developed by Microsoft, known for its advanced capabilities in natural language understanding and generation.
  image_price: null
  inference_provider:
    model_name: microsoft/WizardLM-2-8x22B
    provider: togetherai
  input_formats:
  - text
  limits:
    max_context_size: 65536
  model: WizardLM-2-8x22B
  model_provider: togetherai
  output_formats:
  - text
  price:
    per_input_token: 0.3
    per_output_token: 0.3
    valid_from: null
  type: completions
- capabilities: []
  description: LLaMA-2 Chat (13B) is Meta's conversational AI model, designed for engaging and coherent dialogue.
  image_price: null
  inference_provider:
    model_name: meta-llama/Llama-2-13b-chat-hf
    provider: togetherai
  input_formats:
  - text
  limits:
    max_context_size: 4096
  model: Llama-2-13b-chat-hf
  model_provider: togetherai
  output_formats:
  - text
  price:
    per_input_token: 0.3
    per_output_token: 0.3
    valid_from: null
  type: completions
- capabilities: []
  description: Upstage SOLAR Instruct v1 (11B) is a versatile model by Upstage, focused on following instructions across various domains.
  image_price: null
  inference_provider:
    model_name: upstage/SOLAR-10.7B-Instruct-v1.0
    provider: togetherai
  input_formats:
  - text
  limits:
    max_context_size: 4096
  model: SOLAR-10.7B-Instruct-v1.0
  model_provider: togetherai
  output_formats:
  - text
  price:
    per_input_token: 0.3
    per_output_token: 0.3
    valid_from: null
  type: completions
- capabilities: []
  description: Qwen 2.5 72B Instruct is a large-scale model by Qwen, offering advanced capabilities for complex language tasks.
  image_price: null
  inference_provider:
    model_name: Qwen/Qwen2.5-72B-Instruct-Turbo
    provider: togetherai
  input_formats:
  - text
  limits:
    max_context_size: 32768
  model: Qwen2.5-72B-Instruct-Turbo
  model_provider: togetherai
  output_formats:
  - text
  price:
    per_input_token: 1.2
    per_output_token: 1.2
    valid_from: null
  type: completions
- capabilities: []
  description: DeepSeek-Reasoner is an advanced AI model designed to enhance logical reasoning and problem-solving capabilities, leveraging deep learning techniques to provide accurate and contextually relevant insights across various domains.
  image_price: null
  inference_provider:
    model_name: deepseek-reasoner
    provider: deepseek
  input_formats:
  - text
  limits:
    max_context_size: 64000
  model: deepseek-reasoner
  model_provider: deepseek
  output_formats:
  - text
  price:
    per_input_token: 0.55
    per_output_token: 2.19
    valid_from: null
  type: completions
